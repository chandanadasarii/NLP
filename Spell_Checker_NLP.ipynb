{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Spell Checker - NLP.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/chandanadasarii/NLP/blob/master/Spell_Checker_NLP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "2oP1uun77cIh",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Spell Checker using nltk** :\n",
        "\n",
        "This task will involve the creation of a spellchecking system and an evaluation of its performance. You may use the code snippets provided in Python for completing this or you may use the programming language or environment of your choice\n",
        "\n",
        "Please start by downloading the corpus `holbrook.txt` from Blackboard\n",
        "\n",
        "The file consists of lines of text, with one sentence per line. Errors in the line are marked with a `|` as follows\n",
        "\n",
        "    My siter|sister go|goes to Tonbury .\n",
        "    \n",
        "In this case the word 'siter' was corrected to 'sister' and the word 'go' was corrected to 'goes'.\n",
        "\n",
        "In some places in the corpus two words maybe corrected to a single word or one word to a multiple words. This is denoted in the data using underscores e.g.,\n",
        "\n",
        "    My Mum goes out some_times|sometimes .\n",
        "    \n",
        "For the purpose of this task you do not need to separate these words, but instead you may treat them like a single token.\n",
        "\n",
        "*Note: you may use any functions from NLTK to complete the assignment. It should not be necessary to use other libraries and so please consult with us if your solution involves any other external library. If you use any function from NLTK in Task 6 please include a brief description of this function and how it contributes to your solution.*"
      ]
    },
    {
      "metadata": {
        "id": "TIVCSJV-7kDs",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Step 1 \n",
        "\n",
        "Write a parser that can read all the lines of the file `holbrook.txt` and print out for each line the original (misspelled) text, the corrected text and the indexes of any changes. The indexes refers to the index of the words in the sentence. In the example given, there is only an error in the 10th word and so the list of indexes is [9]. It is not necessary to analyze where the error occurs inside the word.\n",
        "\n",
        "Then split your data into a test set of 100 lines and a training set."
      ]
    },
    {
      "metadata": {
        "id": "p5EdUV7eTXuw",
        "colab_type": "code",
        "outputId": "1102bdf5-536b-4e6b-c15a-92909169d3ef",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        }
      },
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "MX4_0tD4-IKM",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download(\"all\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "RznCZ1mw7mfk",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "readfile = open(\"/content/drive/My Drive/NLP/holbrook.txt\", \"r\")\n",
        "readlines = readfile.readlines()\n",
        "rawdata = [line.replace(\"\\n\",\"\").split(\" \") for line in readlines]\n",
        "char = \"|\" \n",
        "ind = 0\n",
        "key1 = 'original'\n",
        "key2 = 'corrected'\n",
        "key3 = 'indexes'\n",
        "data=[]\n",
        "\n",
        "def correct(rawdata,char):\n",
        "  \n",
        "  for sublist in rawdata:\n",
        "    ind = 0\n",
        "    original=[]\n",
        "    corrected=[]\n",
        "    indexes=[]\n",
        "    data_dict={}\n",
        "    for word in sublist:\n",
        "      if char in word:\n",
        "        split_list = word.split(char) # Split the word based on '|'\n",
        "        original.append(split_list[0])\n",
        "        corrected.append(split_list[1])\n",
        "        indexes.append(ind)\n",
        "      else:\n",
        "        original.append(word)\n",
        "        corrected.append(word)\n",
        "      ind = ind+1\n",
        "      \n",
        "    data_dict[key1]=original\n",
        "    data_dict[key2]=corrected\n",
        "    data_dict[key3]=indexes\n",
        "    data.append(data_dict)\n",
        "    \n",
        "correct(rawdata,char) \n",
        "\n",
        "\n",
        "assert(data[2] == {\n",
        "   'original': ['I', 'have', 'four', 'in', 'my', 'Family', 'Dad', 'Mum', 'and', 'siter', '.'], \n",
        "   'corrected': ['I', 'have', 'four', 'in', 'my', 'Family', 'Dad', 'Mum', 'and', 'sister', '.'], \n",
        "   'indexes': [9]\n",
        "})"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "eRSX4I0H7pSC",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The counts and assertions given in the following sections are based on splitting the training and test set as follows"
      ]
    },
    {
      "metadata": {
        "id": "Kt9aR2Gy7p1C",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "test = data[:100]\n",
        "train = data[100:]\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "hm5JL7cH7sLK",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## **Step 2** : \n",
        "Calculate the frequency (number of occurrences), *ignoring case*, of all words and their unigram probability from the corrected *training* sentences.\n",
        "\n",
        "*Hint: use `Counter` to implement this so it may be called many times*"
      ]
    },
    {
      "metadata": {
        "id": "7ge0uHS-7uEK",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "\n",
        "sent_list=[]\n",
        "for i in range(0,len(train)):\n",
        "  sent_list.append(train[i][\"corrected\"])\n",
        "\n",
        "word_list=[]\n",
        "for sentence in sent_list:\n",
        "  for word in sentence:\n",
        "    word_list.append(word.lower())\n",
        "word_dictionary = Counter(word_list) # word_dictionary has the word and its frequency in a <key,value> format\n",
        "\n",
        "\n",
        "def unigram(word):\n",
        "    word=word.lower()\n",
        "    return (word_dictionary[word])\n",
        "    \n",
        "\n",
        "def prob(word):\n",
        "    word=word.lower()\n",
        "    p = float(unigram(word))/(len(word_list))\n",
        "    return p\n",
        "\n",
        "# Test your code with the following\n",
        "assert(unigram(\"me\")==87)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "w8r8QYj78GPK",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## **Step 3**: \n",
        "[Edit distance](https://en.wikipedia.org/wiki/Edit_distance) is a method that calculates how similar two strings are to one another by counting the minimum number of operations required to transform one string into the other. There is a built-in implementation in NLTK that works as follows:\n"
      ]
    },
    {
      "metadata": {
        "id": "SV9Mu8P38IQE",
        "colab_type": "code",
        "outputId": "f38d6670-d674-48e8-9673-f9a4eeb11fc6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "cell_type": "code",
      "source": [
        "from nltk.metrics.distance import edit_distance\n",
        "\n",
        "# Edit distance returns the number of changes to transform one word to another\n",
        "print(edit_distance(\"hello\", \"hi\"))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Hm46Lbiz8K8M",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Write a function that calculates all words with *minimal* edit distance to the misspelled word. You should do this as follows\n",
        "\n",
        "1. Collect the set of all unique tokens in `train`\n",
        "2. Find the minimal edit distance, that is the lowest value for the function `edit_distance` between `token` and a word in `train`\n",
        "3. Output all unique words in `train` that have this same (minimal) `edit_distance` value\n",
        "\n",
        "*Do not implement edit distance, use the built-in NLTK function `edit_distance`*"
      ]
    },
    {
      "metadata": {
        "id": "HoilAmFW8PCb",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from nltk.metrics.distance import edit_distance\n",
        "dist_dict={}\n",
        "positions = []\n",
        "\n",
        "def get_candidates(token):\n",
        "  min_distance = 1000 #initialization by assuming no word length is >1000 char :P\n",
        "  for word in word_dictionary:\n",
        "      dist = edit_distance(token.lower(),word.lower())\n",
        "      dist_dict[word]=dist #saving the distances of all words which wrt to token in dist_dict<word,distance>\n",
        "  for w,d in dist_dict.items(): \n",
        "      if d == min_distance:\n",
        "        positions.append(w)\n",
        "      if d < min_distance:\n",
        "        min_distance = d\n",
        "        positions = []\n",
        "        positions.append(w)\n",
        "  return positions\n",
        "\n",
        "\n",
        "# Test your code as follows\n",
        "assert get_candidates(\"minde\") == ['mind', 'mine']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "RGY-eCkN8TIM",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Step 4 :\n",
        "\n",
        "Write a function that takes a (misspelled) sentence and returns the corrected version of that sentence. The system should scan the sentence for words that are not in the dictionary (set of unique words in the training set) and for each word that is not in the dictionary choose a word in the dictionary that has minimal edit distance and has the highest *unigram probability*. \n",
        "\n",
        "*Your solution to this should involve `get_candidates`*\n"
      ]
    },
    {
      "metadata": {
        "id": "dIGKE4_P8WGP",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "\n",
        "def correct(sentence):\n",
        "  corrected_sentence=[]\n",
        "  for w in sentence:\n",
        "    if w.lower() in word_dictionary:\n",
        "        corrected_sentence.append(w)\n",
        "    else:\n",
        "        listw=get_candidates(w)# if word is not present in the dictionary call get_candidates method\n",
        "        if(len(listw)==1):\n",
        "            corrected_sentence.append(listw[0]) \n",
        "        elif(len(listw)>1):\n",
        "            dictw = {}\n",
        "            for i in listw:\n",
        "                dictw[i] = prob(i)\n",
        "            c = Counter(dictw)\n",
        "            listw = max(dictw,key=dictw.get) #extracting the word which has max unigram probability\n",
        "            corrected_sentence.append(listw)\n",
        "  \n",
        "  return corrected_sentence        \n",
        "    \n",
        "# print(correct(['NIGEL', 'THRUSH', 'page', '48']))\n",
        "assert(correct([\"this\",\"whitr\",\"cat\"]) == ['this','white','cat'])   "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "oG7jC6au8kka",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## **Step 5** : \n",
        "Using the test corpus evaluate the *accuracy* of your method, i.e., how many words from your system's output match the corrected sentence (you should count words that are already spelled correctly and not changed by the system)."
      ]
    },
    {
      "metadata": {
        "id": "HSXTQypR8mdR",
        "colab_type": "code",
        "outputId": "872f15ac-ed44-40f7-f532-93c943a860f6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        }
      },
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.corpus import wordnet as wn\n",
        "\n",
        "def accuracy(test):\n",
        "  count=0.0\n",
        "  Totall=0.0\n",
        "  for i in range(0,len(test)):\n",
        "    test_sent_orig = test[i][\"original\"]\n",
        "    system_corrected = correct(test_sent_orig)\n",
        "    Totall= Totall+len(test[i][\"corrected\"])\n",
        "    for w,c,o in zip(system_corrected,test[i][\"corrected\"],test[i][\"original\"]):\n",
        "      if(w.lower()==c.lower()):\n",
        "        count = count+1\n",
        "  \n",
        "  print \"Total number of words correctly identified :\",count ,\"out of\",Totall\n",
        "  acc = count/Totall \n",
        "  print \"Accuracy of this model :\" ,(acc*100)\n",
        "  return (acc)\n",
        "\n",
        "accuracy(test)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total number of words correctly identified : 947.0 out of 1129.0\n",
            "Accuracy of this model : 83.8795394154\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8387953941541186"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "metadata": {
        "id": "yL0mVqgSVYe6",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "Overall accuracy with the above system using Unigram probabilities and edit distance is **83.87**. I can clearly notice that many words are misclassified  due to \n",
        "\n",
        "1.   Less training data\n",
        "2.   Tense mismatch or singular/Plural mismatch\n",
        "       for ex : kill/killed ; saturday/saturdays ; sunday/sundays ;races/race\n",
        "3.    Named entities which needs a special attentions. for named entities also our model is finding the nearest words and calculating unigram probability. which is meaningless\n",
        "For example : \n",
        "for the word \"NIGEL\" we are getting \"night\"\n",
        "similary for \"THRUSH\"--through; BBC--abc; I.T.V-tv; Moore--more; Babra;Brinner-dinner; Anglia--again; \n",
        "\n",
        "4.   For some words which are already correct as its not present in the training data it is replacing with other nearest word.\n",
        "for ex: wean with jean; sang with gang; joy with boy; sky with say ; yule with hole\n",
        "\n",
        "5.   Numerical digits also needs a special attention compared to the other words\n",
        "6.  **The above model is not capturing the semantics of the sentences which plays vital role for correction of sentence**\n",
        "\n",
        "Above all are important factors which are effecting  the accuarcy of our sentence correction system. My improved algorithm will systamatically takes care of all the factors mentioned here.  **\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "9b-r2JzD8_Zh",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## **Step 6 :**\n",
        "\n",
        "Consider a modification to your algorithm that would improve the accuracy of the algorithm developed in Task 3 and 4\n",
        "\n",
        "* You may resources beyond those provided here.\n",
        "* You must **not use the test data** in this task.\n",
        "* Provide a short text describing what you intend to do and why. \n",
        "* Full marks for this section may be obtained without an implementation, but an implementation is preferred.\n",
        "* Your implementation should not consist of more than 50 lines of code\n",
        "\n",
        "Please note this task is marked according to: demonstration of knowledge from the lecutures (10), originality and appropriateness of solution (10), completeness of description (10) and technical correctness (5)\n"
      ]
    },
    {
      "metadata": {
        "id": "Ap_PxOCs0RSF",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Improvised Algorithm :\n",
        "\n",
        "\n",
        "\n",
        "**1. Lemmatization :**\n",
        "* Used lemmatization on words to get its base word and check the new lemmatized word is present in training data or words.words() - Reason for this is words may appear in different forms and there is possibility that they may not be present in training data and wordnet. However, it is valid word. Hence to detect this - I used lemmatization.\n",
        "\n",
        "code snippet :\n",
        "\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "\n",
        "\n",
        "**2. Digit/Number detection : **\n",
        "\n",
        "\n",
        "* Ignoring the numbers (dont correct the numbers like page number and date etc.). If this is not implemented the number in test data is replaced with most nearest number in training data to avoid this we need to ignore ditigits and dates.\n",
        "\n",
        "*  With the help of Part of Speech (pos) tagging idntified the numbers/digits.\n",
        "   \n",
        "   nltk.pos_tag(sentence) gives the tags for all the words present in the sentence\n",
        "\n",
        "   if tag(word) == \"CD\" that means word is a number. \n",
        "\n",
        "**3.Named Entity Reconginition : **\n",
        "\n",
        "*Used nltk library for detecting named entities. This is very important as earlier our correcting algorithm ignored the named entities and got the most nearest word with the help of get_candidates to correct the given word.\n",
        "\n",
        "For identifying NamedEntities i have used ne_chunk from nltk package.\n",
        "\n",
        "\"isnamed_entity(word)\" method which i defined below returns True if it is a named entity else it returns False.\n",
        "\n",
        "For example :\n",
        "\n",
        "NIGEL THRUSH page 48\", is \"night through page 48\", because word NIGEL and THRUSH is not present in training data. After implementing of named entities NIGEL THRUSH is recognised as named entities. Hence the predicted sentence is \"NIGEL THRUSH page 48\".\n",
        "\n",
        "**4.Proper Noun Identification :\n",
        "\n",
        "*  With the help of POS tagging identified the named entities which are proper nouns with the help of tag NNP\n",
        "\n",
        "**5. Valid word or not :**\n",
        "\n",
        "* Used wordnet.words()  from nltk.corpus package to see if the word is valid word.\n",
        "Reason for this is words may appear in different forms(past tense/ present tense/ future tense) and there is possibility that they may not be present in training data and wordnet even though it is a valid word. \n",
        "    \n",
        "**6.     Bigram probabilities** :\n",
        "* Using bigrams and conditional frequency distribution I am trying to capture the semantics of the sentence to one level above.\n",
        "\n",
        "for ex : bigrams(\"police\",\"came\") = 3 that means these two words occured together 3 times in order.\n",
        "bigram_prob(\"police\",\"came\")= 0.06 tells that the word \"came\" conditional probability after the word \"police\" \n",
        "based on this bigrams and bigram probability we can capture the semantics of the sentence.\n",
        "\n",
        "\n",
        "with all the above additions my new model giving the siginificant improvement over the previous model.\n",
        "\n",
        "**Accuracy of my new model is : 91.76\n",
        "Total number of words correctly identified : 1036.0 out of 1129.0**\n",
        "\n",
        "**Further improvements:**\n",
        "\n",
        "1. Use of higher order ngrams (for example trigrams), we can capture the sentence semantics better.\n",
        "2. Collecting large amount of training data and training our model on this data.\n",
        "3. Considering the grammer of the sentence (for example tense of the sentence): Need to research in this area for advanced techniques."
      ]
    },
    {
      "metadata": {
        "id": "fbHx_j3NVIpM",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        ""
      ]
    },
    {
      "metadata": {
        "id": "wT76F_dy7gDX",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from nltk.util import ngrams\n",
        "from nltk.probability import ConditionalFreqDist\n",
        "# These methods are for getting bigrams and bigram probabilities\n",
        "sent_list=[]\n",
        "for i in range(0,len(train)):\n",
        "  sent_list.append(train[i][\"corrected\"])\n",
        "training=[]\n",
        "for sentence in sent_list:\n",
        "  for word in sentence:\n",
        "    training.append(word.lower())\n",
        "bigrams = nltk.bigrams(training)\n",
        "\n",
        "cfd = ConditionalFreqDist(bigrams)\n",
        "\n",
        "def bigrams(prev_word, cur_word):\n",
        "  count = 0\n",
        "  if cfd[prev_word].get(cur_word) is not None:\n",
        "    count = cfd[prev_word].get(cur_word)\n",
        "  return count\n",
        "\n",
        "def bigram_prob(prev_word, cur_word):\n",
        "  return float(bigrams(prev_word, cur_word))/(sum(cfd[prev_word].values())+1)\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "gtIMlztg8PGO",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from nltk import ne_chunk\n",
        "# this method is for checking named entities\n",
        "def isnamed_entity(word):\n",
        "  word_temp=[]\n",
        "  word_temp.append(word)\n",
        "  entity = nltk.ne_chunk(nltk.pos_tag(word_temp))\n",
        "  for chunk in entity:\n",
        "    if hasattr(chunk,\"label\"):\n",
        "      return True\n",
        "  return False"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "2UL4hUDZ3IU2",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Improvised method for sentence correction \n",
        "import nltk\n",
        "from collections import Counter\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "from nltk.corpus import wordnet as wn\n",
        "  \n",
        "def correct_new(sentence):\n",
        "    corrected_sentence = []\n",
        "    sentence_postag = nltk.pos_tag(sentence)\n",
        "    index = 0\n",
        "    for (w,tag) in sentence_postag:\n",
        "      if tag == \"NNP\" or tag == \"CD\" or isnamed_entity(w.lower()) or(lemmatizer.lemmatize(w.lower()) in word_dictionary) or (lemmatizer.lemmatize(w.lower()) in wn.words()) or (w.lower() in word_dictionary) or w.lower() in wn.words():\n",
        "            corrected_sentence.append(w)\n",
        "      else:\n",
        "            listw=get_candidates(w)\n",
        "            if index < len(sentence):\n",
        "              next_tag = sentence[index+1]\n",
        "              index += 1\n",
        "            else:\n",
        "              next_tag = sentence[index]  \n",
        "            if(len(listw)==1):\n",
        "                corrected_sentence.append(listw[0])\n",
        "            elif(len(listw)>1):\n",
        "                dictw = {}\n",
        "                for i in listw:\n",
        "                    dictw[i] = bigram_prob(i, next_tag)\n",
        "                c=Counter(dictw)\n",
        "                listw = max(dictw,key=dictw.get)\n",
        "                corrected_sentence.append(listw)\n",
        "  \n",
        "                \n",
        "    return corrected_sentence\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "GLzaC6D28sK9",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## **Task 7 (5 Marks):**\n",
        "\n",
        "Repeat the evaluation (as in Task 5) of your new algorithm and show that it outperforms the algorithm from Task 3 and 4"
      ]
    },
    {
      "metadata": {
        "id": "Hw6PzwWn7iEo",
        "colab_type": "code",
        "outputId": "e4d01079-6443-42cc-9775-74675b6e1282",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        }
      },
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.corpus import wordnet as wn\n",
        "\n",
        "def accuracy_new(test):\n",
        "    count=0.0\n",
        "    Totall=0.0\n",
        "    for i in range(0,len(test)):\n",
        "      test_sent_orig = test[i][\"original\"]\n",
        "      system_corrected = correct_new(test_sent_orig)\n",
        "      Totall= Totall+len(test[i][\"corrected\"])\n",
        "      for w,c,o in zip(system_corrected,test[i][\"corrected\"],test[i][\"original\"]):\n",
        "        if(w.lower()==c.lower()):\n",
        "          count = count+1\n",
        "    print \"Total number of words correctly identified :\",count ,\"out of\",Totall\n",
        "    acc = count/Totall \n",
        "    print \"Accuracy of this model :\" ,(acc*100)\n",
        "    return (acc)\n",
        "\n",
        "print(accuracy_new(test))\n",
        "             \n",
        "          \n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total number of words correctly identified : 1036.0 out of 1129.0\n",
            "Accuracy of this model : 91.7626217892\n",
            "0.917626217892\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "cQ74_AI27tuo",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Results\n",
        "\n",
        "**Earlier Algorithm:**\n",
        "\n",
        "Accuracy : 93.87\n",
        "\n",
        "Total number of words correctly identified : 947.0 out of 1129.0\n",
        "\n",
        "**New Improvised Algorithm:**\n",
        "\n",
        "Accuracy : 91.76\n",
        "\n",
        "Total number of words correctly identified : 1036.0 out of 1129.0"
      ]
    }
  ]
}