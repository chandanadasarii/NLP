{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ANLP_Assignment1.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/chandanadasarii/NLP/blob/master/ANLP_Assignment1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "3J6V9_v5qu2o",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "  # Advanced Natural Language Processing\n",
        "  \n",
        "  ###  Chandana"
      ]
    },
    {
      "metadata": {
        "id": "qndnYAQpL6I8",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "In this task you will develop a system to detect irony in text. We will use the data from the SemEval-2018 task on irony detection. You should use the file `SemEval2018-T3-train-taskA.txt` from Blackboard it consists of examples as follows:"
      ]
    },
    {
      "metadata": {
        "id": "vEvckziyL6I9",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "```csv\n",
        "Tweet index     Label   Tweet text\n",
        "1       1       Sweet United Nations video. Just in time for Christmas. #imagine #NoReligion  http://t.co/fej2v3OUBR\n",
        "2       1       @mrdahl87 We are rumored to have talked to Erv's agent... and the Angels asked about Ed Escobar... that's hardly nothing    ;)\n",
        "3       1       Hey there! Nice to see you Minnesota/ND Winter Weather \n",
        "4       0       3 episodes left I'm dying over here\n",
        "```\n"
      ]
    },
    {
      "metadata": {
        "id": "oc7hUIIjL6I9",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Task 1 \n",
        "\n",
        "Read all the data and find the size of vocabulary of the dataset (ignoring case) and the number of positive and negative examples."
      ]
    },
    {
      "metadata": {
        "id": "vDlHBqT3399R",
        "colab_type": "code",
        "outputId": "a377f969-c8e3-4f9e-9087-b028af4a46aa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        }
      },
      "cell_type": "code",
      "source": [
        "# importing the necessary packages\n",
        "\n",
        "import pandas as pd\n",
        "import nltk\n",
        "from nltk.tokenize import sent_tokenize\n",
        "from nltk.tokenize import word_tokenize\n",
        "nltk.download('punkt')\n",
        "from sklearn.model_selection import train_test_split\n",
        "nltk.download('wordnet')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "metadata": {
        "id": "ZkmmJZ824HE-",
        "colab_type": "code",
        "outputId": "c77e7798-5cec-47ca-afa8-ec54107581f7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        }
      },
      "cell_type": "code",
      "source": [
        "# Loading the data\n",
        "data = pd.read_csv('SemEval2018-T3-train-taskA.txt',sep='\\t')\n",
        "\n",
        "# intializations\n",
        "positive_count=0 \n",
        "negative_count=0\n",
        "full_vocab=[]\n",
        "full_vocab_dict={}\n",
        "\n",
        "# Number of ironic(positive) and non-ironic(negative) example count\n",
        "\n",
        "for label in data['Label']:\n",
        "  if label==1:\n",
        "    positive_count+=1\n",
        "  else:\n",
        "    negative_count+=1\n",
        "  \n",
        "# parsing each tweet to build the corpus by lowering the case and tokenizing the tweets\n",
        "for index,row in data.iterrows():\n",
        "  full_vocab+= word_tokenize(row['Tweet text'].lower())\n",
        "  \n",
        "# iterating over all the words and building a {word - freq} dictionary  \n",
        "for item in full_vocab:\n",
        "  if item in full_vocab_dict :\n",
        "    full_vocab_dict[item] += 1\n",
        "  else:\n",
        "    full_vocab_dict[item] = 1 \n",
        "    \n",
        "print(\"Number of Positive Examples : \"+ str(positive_count))\n",
        "print(\"Number of Negative Examples : \"+ str(negative_count))\n",
        "print(\"Size of the vocabulary : \" + str(len(full_vocab_dict)))\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of Positive Examples : 1901\n",
            "Number of Negative Examples : 1916\n",
            "Size of the vocabulary : 13460\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "qVdS5s6yL6I-",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Task 2\n",
        "\n",
        "Develop a classifier using the Naive Bayes model to predict if an example is ironic. The model should convert each Tweet into a bag-of-words and calculate\n",
        "\n",
        "$p(\\text{Ironic}|w_1,\\ldots,w_n) \\propto \\prod_{i=1,\\ldots,n} p(w_i \\in \\text{tweet}| \\text{Ironic}) p(\\text{Ironic})$\n",
        "\n",
        "$p(\\text{NotIronic}|w_1,\\ldots,w_n) \\propto \\prod_{i=1,\\ldots,n} p(w_i \\in \\text{tweet}| \\text{NotIronic}) p(\\text{NotIronic})$\n",
        "\n",
        "You should use add-alpha smoothing to calculate probabilities"
      ]
    },
    {
      "metadata": {
        "id": "4jr6CWQIFbxZ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Steps involved in Naive Bayes Classifier :**\n",
        "\n",
        "1. Convert the tweet to lower case.\n",
        "2. Tokenize the tweets into words.\n",
        "3. Build the Ironic and Non Ironic Vocabulary list\n",
        "4. Based on the Vocabulary list , build the Ironic and not Ironic dictionaries which captures the frequency of each words in respective classes/contexts.\n",
        "5. Calculate the prior Probabilities of both the classes\n",
        "6. Using the above mentioned formula , calculate the posterior probabilities of both the classes given tweet. and assign the maximum probability class as the label of that tweet.\n",
        "\n",
        "**Advantages **\n",
        "\n",
        "1. Simple and efficient\n",
        "2. Purely based on the word frequency count\n",
        "\n",
        "**Disadvantages**\n",
        "\n",
        "Fails to captures the semantics of the tweets.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "mlUO6Kpr4Nx3",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class NaiveBayes(): \n",
        "  \n",
        "  def __init__(self): #initializations   \n",
        "    self.tweet_words=[]\n",
        "    self.Ironic_vocab=[]\n",
        "    self.notIronic_vocab=[]\n",
        "    self.Ironic_dict={}\n",
        "    self.notIronic_dict={}\n",
        "    self.positive_count=0\n",
        "    self.negative_count=0   \n",
        "    self.Prob_notIronic=0\n",
        "    self.Prob_Ironic=0\n",
        "\n",
        "  def model(self, train):\n",
        "\n",
        "    #Building the Ironic and notIronic Vocabularies  \n",
        "    for index,row in train.iterrows():\n",
        "      if row['Label'] ==1:\n",
        "        self.Ironic_vocab+=word_tokenize(row['Tweet text'].lower())\n",
        "      else:\n",
        "        self.notIronic_vocab+=word_tokenize(row['Tweet text'].lower())\n",
        "\n",
        "    #Ironic and notIronic dictionaries \n",
        "    for item in self.notIronic_vocab:\n",
        "      if item in self.notIronic_dict :\n",
        "        self.notIronic_dict[item] += 1\n",
        "      else:\n",
        "        self.notIronic_dict[item] = 1\n",
        "    for item in self.Ironic_vocab:\n",
        "      if item in self.Ironic_dict :\n",
        "        self.Ironic_dict[item] += 1\n",
        "      else:\n",
        "        self.Ironic_dict[item] = 1\n",
        "\n",
        "    # Ironic and notIronic Label count\n",
        "    for label in train['Label']:\n",
        "      if label==1:\n",
        "        self.positive_count+=1\n",
        "      else:\n",
        "        self.negative_count+=1\n",
        "\n",
        "    # Calculating prior probabilites for Ironic and notIronic classes\n",
        "    self.Prob_Ironic = self.positive_count/(self.positive_count+self.negative_count)\n",
        "    self.Prob_notIronic = self.negative_count/(self.positive_count+self.negative_count)\n",
        "\n",
        "  def predict(self,test):\n",
        "    count = 0\n",
        "    for tweet,label in zip(test['Tweet text'],test['Label']):\n",
        "      tweet_words=word_tokenize(tweet.lower())\n",
        "      prob_Ironic_given_tweet = 1\n",
        "      prob_notIronic_given_tweet = 1\n",
        "\n",
        "      for word in tweet_words:\n",
        "        if word not in self.Ironic_dict:\n",
        "          self.Ironic_dict[word]=0\n",
        "        if word not in self.notIronic_dict:          \n",
        "          self.notIronic_dict[word]=0\n",
        "        #Calculating likelihoods\n",
        "        prob_Ironic_given_tweet = prob_Ironic_given_tweet*(self.Ironic_dict[word]+1)/(len(self.Ironic_vocab)+len(self.Ironic_dict))\n",
        "        prob_notIronic_given_tweet = prob_notIronic_given_tweet*(self.notIronic_dict[word]+1)/(len(self.notIronic_vocab)+len(self.notIronic_dict))\n",
        "      \n",
        "      # Calculating Posterior probability of Ironic given tweet\n",
        "      prob_Ironic_given_tweet = prob_Ironic_given_tweet * self.Prob_Ironic\n",
        "      # Calculating Posterior probability of notIronic given tweet\n",
        "      prob_notIronic_given_tweet = prob_notIronic_given_tweet * self.Prob_notIronic\n",
        "\n",
        "      # based on maximum likelihood probability assigning the labels\n",
        "      if prob_notIronic_given_tweet > prob_Ironic_given_tweet:\n",
        "        NB_label = 0\n",
        "      else:\n",
        "        NB_label = 1\n",
        "      #checking whether the actual label and NB predicted label are same\n",
        "      if label==NB_label:\n",
        "        count+=1\n",
        "      \n",
        "          \n",
        "    #Calculating the accuracy      \n",
        "    accuracy= count/len(test)\n",
        "    print(\"Accuracy of NaiveBayes : {:0.2f}%\\n\".format(accuracy*100))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "w5LivWxOL6JA",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Task 3\n",
        "\n",
        "Divide the data into a training and test set and justify your split.\n",
        "\n",
        "Choose a suitable evaluation metric and implement it. Explain why you chose this evaluation metric.\n",
        "\n",
        "Evaluate the method in Task 2 according to this metric."
      ]
    },
    {
      "metadata": {
        "id": "4wcUnIJc4T6X",
        "colab_type": "code",
        "outputId": "3f499ceb-7712-412a-f9ce-cc300df99d85",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "cell_type": "code",
      "source": [
        "# train - test split\n",
        "train_raw, test_raw = train_test_split(data, test_size=0.25, random_state=42)\n",
        "\n",
        "# created one more copy of train test to use ahead\n",
        "train_lstm_imp = train_raw\n",
        "test_lstm_imp  = test_raw\n",
        "# Naive Bayes model create and predict\n",
        "nb = NaiveBayes()\n",
        "nb.model(train_raw)\n",
        "nb.predict(test_raw)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy of NaiveBayes : 64.71%\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "82JnhmgBL6JA",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Task 4\n",
        "\n",
        "Run the following code to generate a model from your training set. The training set should be in a variable  called `train` and is assumed to be of the form:\n",
        "\n",
        "```\n",
        "[(1, 1, ['sweet', 'united', 'nations', 'video', '.', 'just', 'in', 'time', 'for', 'christmas', '.', '#', 'imagine', '#', 'noreligion', 'http', ':', '//t.co/fej2v3oubr']), \n",
        " (2, 1, ['@', 'mrdahl87', 'we', 'are', 'rumored', 'to', 'have', 'talked', 'to', 'erv', \"'s\", 'agent', '...', 'and', 'the', 'angels', 'asked', 'about', 'ed', 'escobar', '...', 'that', \"'s\", 'hardly', 'nothing', ';', ')']), \n",
        " (3, 1, ['hey', 'there', '!', 'nice', 'to', 'see', 'you', 'minnesota/nd', 'winter', 'weather']), \n",
        " (4, 0, ['3', 'episodes', 'left', 'i', \"'m\", 'dying', 'over', 'here']), \n",
        " ...\n",
        "]\n",
        " ```\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "RQdNBbsI4jhY",
        "colab_type": "code",
        "outputId": "07f04a85-053a-4f47-a9b4-6aa3161cb6db",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        }
      },
      "cell_type": "code",
      "source": [
        "# converting the data into the above format , which is compatible with the keras model given below\n",
        "# converting each row -> (index,label,[tokens_of_tweet])\n",
        "\n",
        "Train =[]\n",
        "Test=[]\n",
        "for index,row in train.iterrows():\n",
        "  # to convert to this form : (index,label,[tokenized_tweet])\n",
        "  r = (index,row['Label'],word_tokenize(row['Tweet text'].lower())) \n",
        "  Train.append(r)\n",
        "print(Train[1:10])\n",
        "\n",
        "for index,row in test.iterrows():\n",
        "  r = (index,row['Label'],word_tokenize(row['Tweet text'].lower()))\n",
        "  Test.append(r)\n",
        "print(Test[1:10])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[(1044, 0, ['#', 'lebron', '#', 'james', ':', \"'\", '#', 'violence', '#', 'is', '#', 'the', '#', 'answer', \"'\", ':', 'lebron', 'james', 'said', 'thursday', 'that', '``', 'violence', 'is', 'not', 'the', 'answer', 'and', '...', 'http', ':', '//t.co/gpz3d9tiuv']), (2820, 0, ['by', '@', 'imjayded_', '``', ':', 'fire', ':', ':fire', ':', ':fire', ':', '@', 'dipmag', 'gears', 'up', 'to', 'release', 'its', 'next', 'issue', ':', 'soon_with_rightwards_arrow_above', ':', \"'wifey\", 'series', \"'\", 'with', '#', 'covermodels', '...', 'http', ':', '//t.co/dqrkfjmyvl']), (69, 1, ['just', 'delivered', '@', 'dominiqueansel', '#', 'cronuts', 'to', '@', 'bouchonbakeryrc', 'hmmm', 'its', 'for', 'a', 'customer', 'i', 'hope', '!', '!', '!', 'http', ':', '//t.co/kdx0fhvqcw']), (3705, 1, ['i', \"'m\", 'not', 'in', 'enough', 'dance', 'practice', 'whatsapp', 'groups', 'yet', '.', '#', 'december', '#', 'winter', '#', 'weddingseason']), (3136, 0, ['@', 'gemheartbeat', '@', 'mooglexox', 'neither', 'can', 'i', ',', 'i', \"'m\", 'gon', 'na', 'have', 'an', 'ambulance', 'pre-booked', 'gorj', ':', 'purple_heart', ':', ':purple_heart', ':', ':purple_heart', ':']), (1334, 0, ['#', 'notcies', '#', 'eu', 'ebola', 'leaves', 'hundreds', 'of', 'thousands', 'facing', 'hunger', 'in', 'three', 'worst-hit', 'countries', 'http', ':', '//t.co/g79x9se31r']), (195, 0, ['@', 'lionhart32', '@', 'dosta1', '@', 'bsbutcher', 'my', ',', 'nephew', 'stopped', 'a', 'lot', '.', 'his', 'long', 'hair', '.', 'looks', 'like', 'a', 'drug', 'dealer', '.', 'never', 'smoked', 'anything', 'pills', '?', 'not', 'an', 'aspirin', '.']), (532, 1, ['@', 'wecameashailee', ':', '10', 'hour', 'car', 'ride', 'and', 'i', 'cant', 'sleep', 'in', 'cars', 'yayaayayayayay', '.', 'have', 'fun', 'bff', '!']), (3792, 0, ['montana', 'of', '300', '14', '-', 'the', 'best', 'versace', 'remix', 'in', 'the', '...', ':', 'http', ':', '//t.co/saak4k9uzq', 'this', 'dude', 'goes', 'off'])]\n",
            "[(1425, 1, ['taking', 'a', 'final', 'then', 'going', 'straight', 'to', 'work', '.', 'i', \"'m\", 'just', 'peachy', 'about', 'it', '.']), (2185, 1, ['@', 'jamaicavaper', '@', 'hifistud', '@', 'outfrontcnn', '@', 'cnn', 'this', 'has', 'been', 'my', 'point', 'all', 'along', ',', 'that', 'tc', 'is', 'proving', 'to', 'be', '#', 'ecigs', 'best', 'advertiser', 'to', 'teens', '.']), (2519, 1, ['your', 'cyber', 'monday', 'sale', 'got', 'extended', '?', 'no', 'fucking', 'way', '!', '#', 'stfu']), (3045, 0, ['drudgery', 'personified', '#', 'commuterprobs', '#', 'northernline', '#', 'tfl', 'http', ':', '//t.co/sadbkdqisx']), (1113, 0, ['how', 'could', 'he', 'kill', 'is', 'best', 'friend', '#', 'a', 'true', '#', 'friend']), (3105, 0, ['going', 'to', 'my', 'class', '#', 'fuckyou', 'important']), (3213, 0, ['@', 'injuryexpert', 'if', 'i', 'can', 'make', 'it', 'out', 'there', ',', 'i', \"'d\", 'love', 'to', 'have', 'a', 'beer', '(', 'or', 'ten', ')', 'and', 'talk', 'baseball', '.', 'you', \"'re\", 'a', 'good', 'dude', '.']), (3518, 1, ['@', 'oimagenta', 'just', 'abt', '2', 'say', 'd', 'same', ':', ')', 'i', \"'m\", 'not', 'sure', 'whether', 'oxford', 'brookes', 'uni', 'is', 'part', 'of', 'oxford', 'uni', '.', 'yet', 'his', 'cv', 'is', 'impressive', 'still', '!']), (3773, 0, ['i', \"'ve\", 'honestly', 'never', 'even', 'watched', 'a', 'christmas', 'story', '.', 'i', \"'ve\", 'only', 'seen', 'random', 'parts', '.'])]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "iV207juZL6JB",
        "colab_type": "code",
        "outputId": "a5577284-65eb-4511-87aa-a5e4f166c2bf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1533
        }
      },
      "cell_type": "code",
      "source": [
        "from keras.models import Sequential, load_model\n",
        "from keras.layers import Dense, Activation, Embedding, Dropout, TimeDistributed\n",
        "from keras.layers import LSTM\n",
        "from keras.optimizers import Adam\n",
        "from keras.utils import to_categorical\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "import numpy as np\n",
        "\n",
        "## These values should be set from Task 3\n",
        "train, test = Train,Test\n",
        "\n",
        "def make_dictionary(train, test):\n",
        "    dictionary = {}\n",
        "    for d in train+test:\n",
        "        for w in d[2]:\n",
        "            if w not in dictionary:\n",
        "                dictionary[w] = len(dictionary)\n",
        "    return dictionary\n",
        "\n",
        "class KerasBatchGenerator(object):\n",
        "    def __init__(self, data, num_steps, batch_size, vocabulary, skip_step=5):\n",
        "        self.data = data\n",
        "        self.num_steps = num_steps\n",
        "        self.batch_size = batch_size\n",
        "        self.vocabulary = vocabulary\n",
        "        self.current_idx = 0\n",
        "        self.current_sent = 0\n",
        "        self.skip_step = skip_step\n",
        "\n",
        "    def generate(self):\n",
        "        x = np.zeros((self.batch_size, self.num_steps))\n",
        "        y = np.zeros((self.batch_size, self.num_steps, 2))\n",
        "        while True:\n",
        "            for i in range(self.batch_size):\n",
        "                # Choose a sentence and position with at lest num_steps more words\n",
        "                while self.current_idx + self.num_steps >= len(self.data[self.current_sent][2]):\n",
        "                    self.current_idx = self.current_idx % len(self.data[self.current_sent][2])\n",
        "                    self.current_sent += 1\n",
        "                    if self.current_sent >= len(self.data):\n",
        "                        self.current_sent = 0\n",
        "                # The rows of x are set to values like [1,2,3,4,5]\n",
        "                x[i, :] = [self.vocabulary[w] for w in self.data[self.current_sent][2][self.current_idx:self.current_idx + self.num_steps]]\n",
        "                # The rows of y are set to values like [[1,0],[1,0],[1,0],[1,0],[1,0]]\n",
        "                y[i, :, :] = [[self.data[self.current_sent][1], 1-self.data[self.current_sent][1]]] * self.num_steps\n",
        "                self.current_idx += self.skip_step\n",
        "            yield x, y\n",
        "\n",
        "# Hyperparameters for model\n",
        "vocabulary = make_dictionary(train, test)\n",
        "num_steps = 5\n",
        "batch_size = 20\n",
        "num_epochs = 50 # Reduce this if the model is taking too long to train (or increase for performance)\n",
        "hidden_size = 50 # Increase this to improve perfomance (or increase for performance)\n",
        "use_dropout=True\n",
        "\n",
        "# Create batches for RNN\n",
        "train_data_generator = KerasBatchGenerator(train, num_steps, batch_size, vocabulary,\n",
        "                                           skip_step=num_steps)\n",
        "valid_data_generator = KerasBatchGenerator(test, num_steps, batch_size, vocabulary,\n",
        "                                           skip_step=num_steps)\n",
        "\n",
        "# A double stacked LSTM with dropout and n hidden layers\n",
        "model = Sequential()\n",
        "model.add(Embedding(len(vocabulary), hidden_size, input_length=num_steps))\n",
        "model.add(LSTM(hidden_size, return_sequences=True))\n",
        "model.add(LSTM(hidden_size, return_sequences=True))\n",
        "if use_dropout:\n",
        "    model.add(Dropout(0.5))\n",
        "model.add(TimeDistributed(Dense(2)))\n",
        "model.add(Activation('softmax'))\n",
        "\n",
        "# Set optimizer and build model\n",
        "optimizer = Adam()\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['categorical_accuracy'])\n",
        "\n",
        "# Train the model\n",
        "model.fit_generator(train_data_generator.generate(), len(train)//(batch_size*num_steps), num_epochs,\n",
        "                        validation_data=valid_data_generator.generate(),\n",
        "                        validation_steps=len(test)//(batch_size*num_steps))\n",
        "\n",
        "# Save the model\n",
        "model.save(\"final_model.hdf5\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "28/28 [==============================] - 1s 25ms/step - loss: 0.6504 - categorical_accuracy: 0.6186 - val_loss: 0.7388 - val_categorical_accuracy: 0.4744\n",
            "Epoch 9/50\n",
            "28/28 [==============================] - 1s 25ms/step - loss: 0.6223 - categorical_accuracy: 0.6961 - val_loss: 0.6949 - val_categorical_accuracy: 0.5644\n",
            "Epoch 10/50\n",
            "28/28 [==============================] - 1s 25ms/step - loss: 0.6126 - categorical_accuracy: 0.6736 - val_loss: 0.6802 - val_categorical_accuracy: 0.6056\n",
            "Epoch 11/50\n",
            "28/28 [==============================] - 1s 25ms/step - loss: 0.5943 - categorical_accuracy: 0.7021 - val_loss: 0.7491 - val_categorical_accuracy: 0.5322\n",
            "Epoch 12/50\n",
            "28/28 [==============================] - 1s 25ms/step - loss: 0.5657 - categorical_accuracy: 0.7129 - val_loss: 0.7269 - val_categorical_accuracy: 0.5744\n",
            "Epoch 13/50\n",
            "28/28 [==============================] - 1s 25ms/step - loss: 0.5482 - categorical_accuracy: 0.7339 - val_loss: 0.7995 - val_categorical_accuracy: 0.5200\n",
            "Epoch 14/50\n",
            "28/28 [==============================] - 1s 26ms/step - loss: 0.4894 - categorical_accuracy: 0.7757 - val_loss: 0.9424 - val_categorical_accuracy: 0.4889\n",
            "Epoch 15/50\n",
            "28/28 [==============================] - 1s 24ms/step - loss: 0.4692 - categorical_accuracy: 0.7854 - val_loss: 0.8834 - val_categorical_accuracy: 0.5411\n",
            "Epoch 16/50\n",
            "28/28 [==============================] - 1s 26ms/step - loss: 0.4599 - categorical_accuracy: 0.7911 - val_loss: 0.8281 - val_categorical_accuracy: 0.5800\n",
            "Epoch 17/50\n",
            "28/28 [==============================] - 1s 25ms/step - loss: 0.4098 - categorical_accuracy: 0.8071 - val_loss: 0.9369 - val_categorical_accuracy: 0.5633\n",
            "Epoch 18/50\n",
            "28/28 [==============================] - 1s 26ms/step - loss: 0.4217 - categorical_accuracy: 0.7900 - val_loss: 0.9811 - val_categorical_accuracy: 0.5178\n",
            "Epoch 19/50\n",
            "28/28 [==============================] - 1s 24ms/step - loss: 0.4136 - categorical_accuracy: 0.8089 - val_loss: 0.8363 - val_categorical_accuracy: 0.5633\n",
            "Epoch 20/50\n",
            "28/28 [==============================] - 1s 25ms/step - loss: 0.3688 - categorical_accuracy: 0.8150 - val_loss: 1.0059 - val_categorical_accuracy: 0.5289\n",
            "Epoch 21/50\n",
            "28/28 [==============================] - 1s 25ms/step - loss: 0.3544 - categorical_accuracy: 0.8279 - val_loss: 1.1982 - val_categorical_accuracy: 0.4678\n",
            "Epoch 22/50\n",
            "28/28 [==============================] - 1s 25ms/step - loss: 0.3757 - categorical_accuracy: 0.8214 - val_loss: 0.9033 - val_categorical_accuracy: 0.5578\n",
            "Epoch 23/50\n",
            "28/28 [==============================] - 1s 25ms/step - loss: 0.3375 - categorical_accuracy: 0.8382 - val_loss: 1.0538 - val_categorical_accuracy: 0.5911\n",
            "Epoch 24/50\n",
            "28/28 [==============================] - 1s 26ms/step - loss: 0.3297 - categorical_accuracy: 0.8311 - val_loss: 1.1218 - val_categorical_accuracy: 0.5189\n",
            "Epoch 25/50\n",
            "28/28 [==============================] - 1s 26ms/step - loss: 0.3663 - categorical_accuracy: 0.8239 - val_loss: 0.9189 - val_categorical_accuracy: 0.5278\n",
            "Epoch 26/50\n",
            "28/28 [==============================] - 1s 25ms/step - loss: 0.3296 - categorical_accuracy: 0.8218 - val_loss: 0.9345 - val_categorical_accuracy: 0.5889\n",
            "Epoch 27/50\n",
            "28/28 [==============================] - 1s 25ms/step - loss: 0.2878 - categorical_accuracy: 0.8479 - val_loss: 1.5017 - val_categorical_accuracy: 0.4689\n",
            "Epoch 28/50\n",
            "28/28 [==============================] - 1s 25ms/step - loss: 0.3659 - categorical_accuracy: 0.8286 - val_loss: 0.9873 - val_categorical_accuracy: 0.5422\n",
            "Epoch 29/50\n",
            "28/28 [==============================] - 1s 26ms/step - loss: 0.3179 - categorical_accuracy: 0.8411 - val_loss: 1.0416 - val_categorical_accuracy: 0.5656\n",
            "Epoch 30/50\n",
            "28/28 [==============================] - 1s 25ms/step - loss: 0.3030 - categorical_accuracy: 0.8418 - val_loss: 1.1031 - val_categorical_accuracy: 0.5744\n",
            "Epoch 31/50\n",
            "28/28 [==============================] - 1s 26ms/step - loss: 0.3400 - categorical_accuracy: 0.8279 - val_loss: 0.9885 - val_categorical_accuracy: 0.5656\n",
            "Epoch 32/50\n",
            "28/28 [==============================] - 1s 26ms/step - loss: 0.2883 - categorical_accuracy: 0.8454 - val_loss: 0.9931 - val_categorical_accuracy: 0.5467\n",
            "Epoch 33/50\n",
            "28/28 [==============================] - 1s 25ms/step - loss: 0.2911 - categorical_accuracy: 0.8379 - val_loss: 1.1878 - val_categorical_accuracy: 0.5389\n",
            "Epoch 34/50\n",
            "28/28 [==============================] - 1s 25ms/step - loss: 0.3104 - categorical_accuracy: 0.8471 - val_loss: 1.2120 - val_categorical_accuracy: 0.4844\n",
            "Epoch 35/50\n",
            "28/28 [==============================] - 1s 25ms/step - loss: 0.3131 - categorical_accuracy: 0.8336 - val_loss: 1.0352 - val_categorical_accuracy: 0.5833\n",
            "Epoch 36/50\n",
            "28/28 [==============================] - 1s 25ms/step - loss: 0.2756 - categorical_accuracy: 0.8561 - val_loss: 1.2077 - val_categorical_accuracy: 0.5822\n",
            "Epoch 37/50\n",
            "28/28 [==============================] - 1s 26ms/step - loss: 0.3937 - categorical_accuracy: 0.8189 - val_loss: 0.8785 - val_categorical_accuracy: 0.5778\n",
            "Epoch 38/50\n",
            "28/28 [==============================] - 1s 25ms/step - loss: 0.2988 - categorical_accuracy: 0.8514 - val_loss: 0.9552 - val_categorical_accuracy: 0.5433\n",
            "Epoch 39/50\n",
            "28/28 [==============================] - 1s 26ms/step - loss: 0.2874 - categorical_accuracy: 0.8418 - val_loss: 1.0636 - val_categorical_accuracy: 0.5689\n",
            "Epoch 40/50\n",
            "28/28 [==============================] - 1s 26ms/step - loss: 0.2890 - categorical_accuracy: 0.8450 - val_loss: 1.3724 - val_categorical_accuracy: 0.4744\n",
            "Epoch 41/50\n",
            "28/28 [==============================] - 1s 26ms/step - loss: 0.3531 - categorical_accuracy: 0.8239 - val_loss: 1.0177 - val_categorical_accuracy: 0.5500\n",
            "Epoch 42/50\n",
            "28/28 [==============================] - 1s 25ms/step - loss: 0.2702 - categorical_accuracy: 0.8643 - val_loss: 1.1570 - val_categorical_accuracy: 0.5656\n",
            "Epoch 43/50\n",
            "28/28 [==============================] - 1s 26ms/step - loss: 0.3037 - categorical_accuracy: 0.8443 - val_loss: 1.1093 - val_categorical_accuracy: 0.5711\n",
            "Epoch 44/50\n",
            "28/28 [==============================] - 1s 26ms/step - loss: 0.2889 - categorical_accuracy: 0.8418 - val_loss: 1.0310 - val_categorical_accuracy: 0.5467\n",
            "Epoch 45/50\n",
            "28/28 [==============================] - 1s 26ms/step - loss: 0.2765 - categorical_accuracy: 0.8404 - val_loss: 1.0990 - val_categorical_accuracy: 0.5500\n",
            "Epoch 46/50\n",
            "28/28 [==============================] - 1s 25ms/step - loss: 0.2982 - categorical_accuracy: 0.8514 - val_loss: 1.2392 - val_categorical_accuracy: 0.5200\n",
            "Epoch 47/50\n",
            "28/28 [==============================] - 1s 26ms/step - loss: 0.2882 - categorical_accuracy: 0.8468 - val_loss: 1.3683 - val_categorical_accuracy: 0.5111\n",
            "Epoch 48/50\n",
            "28/28 [==============================] - 1s 26ms/step - loss: 0.2695 - categorical_accuracy: 0.8525 - val_loss: 1.1869 - val_categorical_accuracy: 0.5544\n",
            "Epoch 49/50\n",
            "28/28 [==============================] - 1s 27ms/step - loss: 0.2890 - categorical_accuracy: 0.8468 - val_loss: 1.2406 - val_categorical_accuracy: 0.6078\n",
            "Epoch 50/50\n",
            "28/28 [==============================] - 1s 25ms/step - loss: 0.2807 - categorical_accuracy: 0.8446 - val_loss: 1.1167 - val_categorical_accuracy: 0.5378\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "qLYwZTAVL6JH",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Now consider the following code:"
      ]
    },
    {
      "metadata": {
        "id": "wQfP5qylL6JH",
        "colab_type": "code",
        "outputId": "face2d99-7a3d-4b26-f265-54ec9a2ce356",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        }
      },
      "cell_type": "code",
      "source": [
        "model = load_model(\"final_model.hdf5\")\n",
        "\n",
        "x = np.zeros((1,num_steps))\n",
        "x[0,:] = [vocabulary[\"this\"],vocabulary[\"is\"],vocabulary[\"an\"],vocabulary[\"easy\"],vocabulary[\"test\"]]\n",
        "print(model.predict(x))\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[[0.22709972 0.7729003 ]\n",
            "  [0.2189427  0.7810573 ]\n",
            "  [0.3058866  0.69411343]\n",
            "  [0.02666874 0.9733313 ]\n",
            "  [0.19957547 0.8004246 ]]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "zh8vY22gL6JL",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Using the code above write a function that can predict the label using the LSTM model above and compare it with the evaluation performed in Task 3"
      ]
    },
    {
      "metadata": {
        "id": "_O_QhJaW4731",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# function to check the performance of LSTM model\n",
        "def prediction_using_lstm_model(test):\n",
        "  \n",
        "  y_predict = list()\n",
        "  \n",
        "  '''\n",
        "  Above defined LSTM model accepts only 5 words at a time.\n",
        "  to process 5 words in a tweet every time. divide the tweet into chunks, \n",
        "  in such a way that each chunk consist of 5 words.\n",
        "  To achive this, maintained 2 positions current_position and to_position, \n",
        "  after each iteration updating this positions by adding the num_steps\n",
        "  \n",
        "  '''\n",
        "  for row in test:\n",
        "    current_position = 0\n",
        "    to_position=0\n",
        "    p_predict = np.ones(2)\n",
        "    #to find the maximum chunks possible ina tweet\n",
        "    max_chunks = int(len(row[2])/num_steps)\n",
        "    for i in range(max_chunks):\n",
        "      x = np.zeros((1, num_steps))\n",
        "      to_position = to_position+num_steps\n",
        "      # boundary case , after updating to_position if it goes beyond the length of tweet, correcting it by equating it to max_langth of tweet\n",
        "      if to_position > len(row[2]):\n",
        "        to_position = len(row[2])\n",
        "        # and updating the current position\n",
        "        current_position = to_position-num_steps\n",
        "      #processing the chunk   \n",
        "      x[0, :] = [vocabulary[w] for w in row[2][current_position:to_position]]\n",
        "      #predicting using the above lstm model\n",
        "      p_temp = model.predict(x)\n",
        "      #multiplyig the prediction probabilities of all chunks to get the complete tweet probability\n",
        "      p_predict = p_predict * np.prod(p_temp[0], axis=0)\n",
        "      #updating the current position to process the next chunk\n",
        "      current_position += num_steps\n",
        "\n",
        "    #assigning the label based on the maximum probability between 2 classes\n",
        "    y_predict.append(int(p_predict[0] > p_predict[1]))\n",
        "  \n",
        "  return y_predict"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "lhr9z2ygbSjO",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# calling the above method to get the labels\n",
        "y_predict_using_lstm = prediction_using_lstm_model(test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ZMzW1kzd5AOA",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Performance Metrics : Accuracy- Precision - Recall - F Score - Confusion matix\n",
        "\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "def metrics_demo(actual_label,predicted_label): \n",
        "    \n",
        "    result = [i for i,j in zip(actual_label,predicted_label) if i==j]\n",
        "    F1_score = f1_score(actual_label, predicted_label, average=\"macro\")\n",
        "    tn, fp, fn, tp = confusion_matrix(actual_label,predicted_label).ravel()\n",
        "    accuracy = len(result)/len(actual_label)\n",
        "\n",
        "    print(\"---------------------------------\")\n",
        "    print(\"Accuracy of keras LSTM model : {:0.2f}%\\n\".format(accuracy*100))\n",
        "    print(\"F1 Score of keras LSTM model : {:0.2f}\\n\".format(F1_score*100))\n",
        "    print(\"---------------------------------\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Xq03BTFvX6mt",
        "colab_type": "code",
        "outputId": "5660ffec-8868-4046-85c2-80e37decbc43",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "cell_type": "code",
      "source": [
        "# calculate the performance metrics for the above lstm model\n",
        "actual_lab=list()\n",
        "for row in Test:\n",
        "  actual_lab.append(row[1])#actual test labels\n",
        "  \n",
        "# calling the metrics_demo function with actual and predicted labels\n",
        "metrics_demo(actual_lab,y_predict_using_lstm)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "---------------------------------\n",
            "Accuracy of keras LSTM model : 52.88%\n",
            "\n",
            "F1 Score of keras LSTM model : 51.43\n",
            "\n",
            "---------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "rWQZK-DWrqjX",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "9yZucGZmL6JM",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Task 5\n",
        "\n",
        "Suggest an improvement to either the system developed in Task 2 or 4 and show that it improves according to your evaluation metric.\n",
        "\n",
        "Please note this task is marked according to: demonstration of knowledge from the lecutures (10), originality and appropriateness of solution (10), completeness of description (10), technical correctness (5) and improvement in evaluation metric (5)."
      ]
    },
    {
      "metadata": {
        "id": "tERXyzrNfYKn",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "In this section , i am discussing about the improvements made to the above given LSTM model in Task 4.\n",
        "\n",
        "\n",
        "As the data is collected from social media platform like twitter,  The data contains a lot of noise, unnecessary extra words, extra spacings, multiple hashtags. Dealing with twitter data needs a god preprocessing, in such a way to remove the unnecessary characters without losing the context or data.\n",
        "\n",
        "**1. Preprocessing the Tweets :**\n",
        "\n",
        "converting the tweets to lower case\n",
        "\n",
        "Replace the URL's present in the tweets with the word 'URL'\n",
        "\n",
        "Replace the @user_mention with the word 'USER'\n",
        "\n",
        "Removing the hashtags present in a word\n",
        "\n",
        "Stripping the extra empty spaces\n",
        "\n",
        "Checking for the validity of a word\n",
        "\n",
        "Handling emojis , by replacing the emojis with positive or negative words\n",
        "\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "vOe68o3t5Hgc",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import re\n",
        "import sys\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "\n",
        "def preprocess_word(word):\n",
        "    # Remove punctuation\n",
        "    word = word.strip('\\'\"?!,.():;')\n",
        "    # Remove & '\n",
        "    word = re.sub(r'(-|\\')', '', word)\n",
        "    return word\n",
        "\n",
        "\n",
        "def is_valid_word(word):\n",
        "    # Check word begins with an alphabet\n",
        "    return (re.search(r'^[a-zA-Z][a-z0-9A-Z\\._]*$', word) is not None)\n",
        "\n",
        "def preprocess_tweet(tweet):\n",
        "    \n",
        "    processed_tweet = []\n",
        "    # Convert to lower case\n",
        "    tweet = tweet.lower()\n",
        "    # Replaces URLs with the word URL\n",
        "    tweet = re.sub(r'((www\\.[\\S]+)|(https?://[\\S]+))', ' URL ', tweet)\n",
        "    # Replace @handle with the word USER\n",
        "    tweet = re.sub(r'@[\\S]+', 'USER', tweet)\n",
        "    # Replaces #hashtag with hashtag\n",
        "    tweet = re.sub(r'#(\\S+)', r' \\1 ', tweet)\n",
        "    # Replace 2+ dots with space\n",
        "    tweet = re.sub(r'\\.{2,}', ' ', tweet)\n",
        "    # Strip space, \" and ' from tweet\n",
        "    tweet = tweet.strip(' \"\\'')\n",
        "    # Replace emojis with either positive , negative\n",
        "    tweet = handle_emojis(tweet)\n",
        "    # Replace multiple spaces with a single space\n",
        "    tweet = re.sub(r'\\s+', ' ', tweet)\n",
        "    words = tweet.split()\n",
        "\n",
        "    for word in words:\n",
        "        word = preprocess_word(word)\n",
        "        if is_valid_word(word):\n",
        "            word = lemmatizer.lemmatize(word)\n",
        "            processed_tweet.append(word)\n",
        "    return ' '.join(processed_tweet)\n",
        "  \n",
        "def handle_emojis(tweet):\n",
        "    # Smile -- :), : ), :-), (:, ( :, (-:, :')\n",
        "    tweet = re.sub(r'(:\\s?\\)|:-\\)|\\(\\s?:|\\(-:|:\\'\\))', ' positive ', tweet)\n",
        "    # Laugh -- :D, : D, :-D, xD, x-D, XD, X-D\n",
        "    tweet = re.sub(r'(:\\s?D|:-D|x-?D|X-?D)', ' positive ', tweet)\n",
        "    # Love -- <3, :*\n",
        "    tweet = re.sub(r'(<3|:\\*)', ' positive ', tweet)\n",
        "    # Wink -- ;-), ;), ;-D, ;D, (;,  (-;\n",
        "    tweet = re.sub(r'(;-?\\)|;-?D|\\(-?;)', ' positive ', tweet)\n",
        "    # Sad -- :-(, : (, :(, ):, )-:\n",
        "    tweet = re.sub(r'(:\\s?\\(|:-\\(|\\)\\s?:|\\)-:)', ' negative ', tweet)\n",
        "    # Cry -- :,(, :'(, :\"(\n",
        "    tweet = re.sub(r'(:,\\(|:\\'\\(|:\"\\()', ' negative ', tweet)\n",
        "    return tweet"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "qzgkqK4YzN0X",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#preprocessing the tweets:\n",
        "\n",
        "def preprocess_data(data):\n",
        "  processed_data_list=[]\n",
        "  for index,row in data.iterrows():\n",
        "      processed_data_list.append(preprocess_tweet(row[2]))\n",
        "  return processed_data_list\n",
        " "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "J4ftWW-0BN52",
        "colab_type": "code",
        "outputId": "1c119ba3-db83-4c93-d7ec-d48dc480166d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "from keras import models\n",
        "from keras import layers\n",
        "from keras import optimizers\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "#preprocessing the train data\n",
        "train_data = preprocess_data(train_raw)\n",
        "train_labels = train_raw['Label'].values\n",
        "#preprocessing the test data\n",
        "test_data = preprocess_data(test_raw)\n",
        "test_labels = test_raw['Label'].values\n",
        "\n",
        "tokenizer = Tokenizer()\n",
        "#vectorizing the tweets by pre-fitted tokenizer instance\n",
        "tokenizer.fit_on_texts(data['Tweet text'])\n",
        "\n",
        "#Padding the tweets\n",
        "#iterating over each sentence and finding the maximum number of words sentence  \n",
        "max_length = max([len(w.split()) for w in data['Tweet text']])\n",
        "\n",
        "#Vocabulary size\n",
        "vocabulary_size = len(tokenizer.word_index)+1\n",
        "print(vocabulary_size)\n",
        "\n",
        "#converting the text to sequences to that it can be given as input to the network.\n",
        "train_tokens = tokenizer.texts_to_sequences(train_data)\n",
        "test_tokens = tokenizer.texts_to_sequences(test_data)\n",
        "\n",
        "#Padding\n",
        "train_padding = pad_sequences(train_tokens,max_length,padding='post')\n",
        "test_padding = pad_sequences(test_tokens,max_length,padding='post')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "12941\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "NcYRHjuvlHne",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Bidirectional Recurrent Neuaral Networks :** \n",
        "\n",
        "As Bidirectional LSTM's are advanced compared to the tradidtional single layer LSTM. \n",
        "\n",
        "\n",
        "Bidirectional LSTMs train two instead of one LSTMs on the input sequence. The first on the input sequence as-is and the second on a reversed copy of the input sequence. This can provide additional context to the network and result in faster and even fuller learning on the problem."
      ]
    },
    {
      "metadata": {
        "id": "Ec0dcEPHCAh7",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Embedding, LSTM,GRU,Dense,Bidirectional\n",
        "from keras.layers.embeddings import Embedding\n",
        "\n",
        "Embedding_dimensionality = 1000\n",
        "#defining the model\n",
        "model = Sequential()\n",
        "#Embedding has parametres vocabulary of 12941, embedding dimensionality, max lengt of tweet\n",
        "model.add(Embedding(vocabulary_size,Embedding_dimensionality,input_length=max_length))\n",
        "#wrappig the LSTM layer in the bidirectional wrapper generates the two copies of hidden layer to handle the sequence from the straight and reverse order \n",
        "model.add(Bidirectional(LSTM(units=20,dropout=0.2, recurrent_dropout=0.2,return_sequences=False)))\n",
        "model.add(Dense(1,activation='sigmoid'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Sm_Hs8BIkzQ_",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "To build the bidirectional LSTM Network. we use  multiple  hyperparameters, can be tunes in order to achieve good results. and used sigmoid as activation function. "
      ]
    },
    {
      "metadata": {
        "id": "6AQ6cO6RPMfm",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#compile the model\n",
        "model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "3KI0sue2P_CI",
        "colab_type": "code",
        "outputId": "d53246e4-71e3-4809-dfda-1ef57ea04503",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 263
        }
      },
      "cell_type": "code",
      "source": [
        "#summary of the model\n",
        "print(model.summary())"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_2 (Embedding)      (None, 187, 1000)         12941000  \n",
            "_________________________________________________________________\n",
            "bidirectional_1 (Bidirection (None, 40)                163360    \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 1)                 41        \n",
            "=================================================================\n",
            "Total params: 13,104,401\n",
            "Trainable params: 13,104,401\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "vfnBbjm4QMRG",
        "colab_type": "code",
        "outputId": "4563a100-01b0-426b-a937-b442ade9fe2f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 404
        }
      },
      "cell_type": "code",
      "source": [
        "model.fit(train_padding,train_labels,batch_size=100,epochs=10,validation_data=(test_padding,test_labels),verbose=2)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 2862 samples, validate on 955 samples\n",
            "Epoch 1/10\n",
            " - 96s - loss: 0.6885 - acc: 0.5360 - val_loss: 0.6855 - val_acc: 0.5393\n",
            "Epoch 2/10\n",
            " - 90s - loss: 0.5973 - acc: 0.7379 - val_loss: 0.6547 - val_acc: 0.5990\n",
            "Epoch 3/10\n",
            " - 90s - loss: 0.4039 - acc: 0.8382 - val_loss: 0.7399 - val_acc: 0.6209\n",
            "Epoch 4/10\n",
            " - 91s - loss: 0.2347 - acc: 0.9217 - val_loss: 0.8686 - val_acc: 0.6000\n",
            "Epoch 5/10\n",
            " - 90s - loss: 0.1410 - acc: 0.9577 - val_loss: 0.9639 - val_acc: 0.6168\n",
            "Epoch 6/10\n",
            " - 90s - loss: 0.0918 - acc: 0.9762 - val_loss: 1.0662 - val_acc: 0.6000\n",
            "Epoch 7/10\n",
            " - 91s - loss: 0.0631 - acc: 0.9832 - val_loss: 1.1844 - val_acc: 0.6209\n",
            "Epoch 8/10\n",
            " - 92s - loss: 0.0475 - acc: 0.9899 - val_loss: 1.2476 - val_acc: 0.6136\n",
            "Epoch 9/10\n",
            " - 90s - loss: 0.0409 - acc: 0.9892 - val_loss: 1.3312 - val_acc: 0.6105\n",
            "Epoch 10/10\n",
            " - 91s - loss: 0.0378 - acc: 0.9923 - val_loss: 1.4081 - val_acc: 0.6052\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7ff1814810f0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 40
        }
      ]
    },
    {
      "metadata": {
        "id": "BWXKX9VQRvTh",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "Improved_LSTM_Labels = model.predict(test_padding)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "_8aHrSj6cL5P",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#for calculating the accuracy\n",
        "\n",
        "Imp_labels =[]\n",
        "for i in Improved_LSTM_Labels:\n",
        "  if i>0.5:\n",
        "    Imp_labels.append(1)\n",
        "  else:\n",
        "    Imp_labels.append(0) \n",
        "    \n",
        "countt=0\n",
        "for (i,j) in zip(Imp_labels,test_labels):\n",
        "  if i==j:\n",
        "    countt=countt+1\n",
        "#print(countt/len(Imp_labels))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "cydHMI-Yofph",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Conclusion**\n",
        "\n",
        "As given Single layer LSTM is failing to capture the semantics. I am using the keras bidirectional LSTM, as it inturn generates the 2 LSTMs to work on both the directions of the sequence. Thus capturing the context effectively.\n",
        "\n",
        "As there are many hyper parameters involved , we can always tune this parameters based on the application we are interested in.\n",
        "\n",
        "In this case with the Single layer of LSTM , \n",
        "\n",
        " ---------------------------------\n",
        "Accuracy of keras LSTM model : 52.88%\n",
        "\n",
        "F1 Score of keras LSTM model : 51.43\n",
        "\n",
        "---------------------------------\n",
        "\n",
        "In this case with the Single layer of LSTM , \n",
        "\n",
        " ---------------------------------\n",
        "Accuracy of keras LSTM model : 60.79%\n",
        "\n",
        "F1 Score of keras LSTM model : 58.43\n",
        "\n",
        "---------------------------------\n",
        " "
      ]
    },
    {
      "metadata": {
        "id": "zc91u_JLqSak",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Compared to the single layer , bidirectional LSTM is performing better. \n",
        "and by tuning the hyperparameters appropriatelty and giving more and more training data will help to further imrove the performance .\n"
      ]
    },
    {
      "metadata": {
        "id": "P5Gi7iO1r1b7",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "\n",
        "Reference :\n",
        "\n",
        "1. Multiple papers on twitter sentiment analysis\n",
        "2. preprocessing techniques on twitter data.\n",
        "3. Keras documentation\n"
      ]
    }
  ]
}